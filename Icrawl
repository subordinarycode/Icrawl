#! /bin/env python3

import argparse
import logging 
from colorama import Fore
from socket import gethostbyname
from urllib.parse import urlsplit


# custom modules
from modules.lookUp import ip_lookup, shodan_lookup, domain_infomation
from modules.runCrawler import crawler
from modules.updater import update_google_dorks
from modules.banners import get_banner
from modules.setPayload import get_payload
from modules.runDorks import run_search

ROOT_LOGGER = logging.getLogger("Icrawl")

# Colors
green = Fore.GREEN
blue = Fore.BLUE
yellow = Fore.YELLOW
red = Fore.RED
norm = Fore.RESET
cyan = Fore.CYAN

# Boxes
fbox = f"{green}[+]{norm}"
ibox = f"{yellow}[+]{norm}"
error = f"{red}[!]{norm}"


__version__ = "1.0.3"



# parses user arguments
def parse_args():
	examples = """Examples:
python3 Icrawl --update
python3 Icrawl --dork --all --verbose
python3 Icrawl -u <url> --phone --hash --emails --verbose
python3 Icrawl -u <url> -A -r [0-9]{3}-[0-9]{3}-[0-9]{4} -v
python3 Icrawl -u <url> --regex ^\(\d{3}\)\s\d{3}-\d{4} --count 10"""
	
	parser = argparse.ArgumentParser(description=f"Information Crawler\t (Version : {__version__})\nAn osint web scraping and google dork tool", epilog=examples,formatter_class=argparse.RawDescriptionHelpFormatter)
	parser.add_argument("-A", "--all",     action="store_true",  help="Search for everything",                                                    dest="all")
	parser.add_argument("-a", "--address", action="store_true",  help="Search for street addresses (Note: returns false positives)",              dest="address")
	parser.add_argument("-ac", "--access", action="store_true",  help="Search for access tokens",                                                 dest="access")
	parser.add_argument("-au", "--auth",   action="store_true",  help="Search for authentication tokens",                                         dest="auth")	
	parser.add_argument("-c", "--count",   type=int,             help="Maximum number of urls to scrape (Default 100)", metavar="", default=100,  dest="count")
	parser.add_argument('-d', '--dorks',   action="store_true",  help="Run google searches using google dorks",                                   dest="dorks")
	parser.add_argument("-D", "--debug",   action="store_true",  help="Turn on logging",                                                          dest="log")
	parser.add_argument("-e", "--emails",  action="store_true",  help="Search for emails addresses",                                              dest="emails")
	parser.add_argument("-H", "--hash",    action="store_true",  help="Search for password hashes",                                               dest="hash")
	parser.add_argument("-i", "--ip",      action="store_true",  help="Proform an IP geolocation lookup",                                         dest="geo")
	parser.add_argument("-k", "--key",     action="store_true",  help="Search for api keys",                                                      dest="api")
	parser.add_argument("-o", "--output",  type=str,             help="Save output to a file",                         metavar="",  default="",   dest="output")
	parser.add_argument("-p", "--phone",   action="store_true",  help="Search for phone numbers",                                                 dest="phone")
	parser.add_argument("-r", "--regex",   default=[],           help="Use custom regular expressions",      nargs='+', metavar="",               dest="reg")
	parser.add_argument("-s", "--secrets", action="store_true",  help="Search for API keys, Hashes, Access tokens Authentication tokens",         dest="secrets")
	parser.add_argument("-S", "--shodan",  action="store_true",  help="Profrom a shodan lookup for the IP of the url",                            dest="shodan")
	parser.add_argument('-u', '--url',     type=str,             help="Url to start scraping/spidering",              metavar="",  default="",    dest="url")
	parser.add_argument('-U', '--update',  action="store_true",  help="Download the latest google dorks",                                         dest="update")
	parser.add_argument("-v", "--verbose", action="store_true",  help="Verbose output",                                                           dest="verbose")
	parser.add_argument("-V", "--version", action="store_true",  help="Display the current version",                                              dest="version")
	parser.add_argument("-w", "--whois", action="store_true",    help="Proform a whois lookup",                                                   dest="whois")
	args = parser.parse_args()
	
	if args.version:
		print(f"Icrawl : {__version__}")
		exit()

	if args.url == "" and args.dorks == False and args.update == False:
		parser.print_usage()
		ROOT_LOGGER.critical("No run option was given")
		exit(1)

	if args.dorks and args.update:
		ROOT_LOGGER.critical("Can not run google dorks search and update at the same time")
		parser.error("Can not run google dorks search and update at the same time")


	# Making sure user input for custom regexs are all valid regexs before moving on
	if args.reg != []:
		for reg in args.reg:
			try:
				formated_reg = fr"{reg}"
				compile(formated_reg)
			except:
				print(f"{error} Invalid regular expression {red}{reg}{norm}")
				ROOT_LOGGER.critical(f"Invalid regular expression : {reg}")
				exit(1)

	ROOT_LOGGER.debug("User arguments parsed successfully")
	return args

# Print resaults from searches
def resaults(dframes, scraped_url):

	# Print the resaults from the shodan lookup 	
	if args.shodan or args.all:
		if len(general_info) > 0:
			for info in general_info:
				print(f"{fbox} {info} : {general_info[info]}")
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(f"{fbox} {info} : {general_info[info]}\n")

		if len(port_info) > 0:
			for port in port_info:
				print(f"{fbox} {port}")
				print(port_info[port])
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(f"{fbox} {port}\n")
						f.write(f"{port_info[port]}\n")

	# Print urls and dataframes containing information scraped from that url
	if len(dframes) == 0:
		print(f"{error} No resaults found")
		if args.output != "":
			with open(args.output, "a") as f:
				f.write(f"{error} No resaults found\n")
	else:
		for i in dframes:
			for key in i:
				print(f"{fbox} {blue}{key}{norm}")
				print(i[key])
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(f"{blue}{key}{norm}\n")
						f.write(f"{i[key]}\n\n")

	if args.whois or args.all:
		# Print resaults from whois lookup
		if len(domain_info) > 0:
			for i in domain_info:
				print(i)
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(i + "\n")
	
	if args.geo or args.all:	
		# Print resaults from ip geolocation
		if len(ip_info) > 0:
			for key, value in ip_info:
				print(f"{green}[+] {cyan}{key}{norm}: {value}")
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(f"{green}[+] {cyan}{key}{norm}: {value}\n")
							
	# Take user input and print all scraped urls
	if args.verbose:
		answer = input(f"{ibox} Do you want to show all {len(scraped_url)} crawled urls?(Y/N): ")
		if answer.lower() == "y":
			for i in scraped_url:
				print(f"{blue}{i}{norm}")
				
	# Writes scraped urls to a file
	if args.output != "":
		with open(args.output, "a") as f:
			f.write("\n")
			for i in scraped_url:
				f.write(f"{fbox} {blue}{i}{norm}\n")

# Asking the user if they want to scrape googles search resaults
def google_dorks():
#payload = {"dorks" : list(list_of_dorks), "links" : num_of_links, "timeout":timeout}
	found_links = []
	payload = get_payload()
	ROOT_LOGGER.debug("Payload for google dorks was set successfully")
	found_links = run_search(payload=payload["dorks"], num_of_links=payload["links"], timeout=payload["timeout"])
	ROOT_LOGGER.debug(f"Google dork search returned {len(found_links)} resaults")

		
	if len(found_links) != 0:
		print(f"{error} Warning: Scraping google search resaults can be against googles terms of service")	
		CONTINUE = input(f"{error} Do you want to continue?(Y/N): ")			

		if CONTINUE.lower() == "y":
			ROOT_LOGGER.debug(f"Parsing {len(found_links)} google search resaults")
			regex_resaults = []
			scraped_links = []
			banner = get_banner()
			print(banner)
			print(f"{ibox} Icrawl has started")
			print(f"{ibox} Press CTRL-C to stop crawling")
			for link in found_links:
				args.url = link
				new_resaults, scraped_urls = crawler(args, spider=False)
				regex_resaults = regex_resaults + new_resaults
				scraped_links = scraped_links + scraped_urls
				
			resaults(regex_resaults, scraped_links)
		else:			
			ROOT_LOGGER.info(f"Printing found links from google dorks and quitting")
			print()
			for url in found_links:
				print(f"{fbox} {blue}{url}{norm}")
				if args.output:
					with open(args.output, "a") as f:
						f.write(f"{fbox} {blue}{url}\n")
						
			print("Goodbye")
			exit()



if __name__ == '__main__':
	args = parse_args()
	
	# Setting the logging level 
	if args.log:
		logging.basicConfig(filename="Icrawl.log", level=0, format="[%(levelname)s] [%(asctime)s] [%(filename)s] %(message)s")
	else:
		logging.basicConfig(filename="Icrawl.log", level=50, format="[%(levelname)s] [%(asctime)s] [%(filename)s] %(message)s")
		
	# Running the who is lookup
	if args.whois or args.all and not args.dorks and not args.update:
		print(f"{ibox} Running whois lookup")
	
		domain_info = domain_infomation(args.url)

	# Running the IP geolocation lookup	
	if args.geo or args.all and not args.dorks and not args.update:
		parts = urlsplit(args.url)
		url = f"{parts.netloc}"
		

		print(f"{ibox} Running IP geolocation lookup")

		domain_ip = gethostbyname(url)
		ip_info = ip_lookup(domain_ip)

	# Running the shodan lookup
	if args.shodan or args.all and not args.dorks and not args.update:
		print(f"{ibox} Running shodan lookup")
		parts = urlsplit(args.url)
		url = f"{parts.netloc}"
		domain_ip = gethostbyname(url)
		general_info, port_info = shodan_lookup(domain_ip)

	# Running the dork update function			
	if args.update:
		downloaded_dorks = update_google_dorks()
		ROOT_LOGGER.info(f"Google dorks updated successfully {downloaded_dorks} where downloaded")
		if downloaded_dorks == None:
			downloaded_dorks = 0
			print(f"[!] {downloaded_dorks} google dorks where downloaded")
		else:
			print(f"\n[+] {downloaded_dorks} google dorks where successfully downloaded")
	
	# Running the google dorks scraper
	elif args.dorks:
		google_dorks()
	
	# Running the web crawler
	else: 

		banner = get_banner()
		print(banner)
		print(f"{ibox} Icrawl has started")
		print(f"{ibox} Press CTRL-C to stop crawling")
		new_resaults, scraped_urls = crawler(args, True)
		resaults(new_resaults, scraped_urls)



