#! /bin/env python3

import argparse
import logging 
from colorama import Fore
from socket import gethostbyname


# custom modules
from modules import dork_update
from modules import run_crawler
from modules import run_dorks
from modules.LookUp import Lookup
from modules import dork_payload
from modules import banners



ROOT_LOGGER = logging.getLogger("Icrawl")
logging.basicConfig(filename="Icrawl.log", level=5, format="[%(levelname)s] [%(asctime)s] [%(filename)s] %(message)s")


# Colors
green = Fore.GREEN
blue = Fore.BLUE
yellow = Fore.YELLOW
red = Fore.RED
norm = Fore.RESET
cyan = Fore.CYAN

# Boxes
fbox = f"{green}[+]{norm}"
box = f"[+]"
ibox = f"{yellow}[+]{norm}"
error = f"{red}[!]{norm}"


__version__ = "1.0.2"



# parses user arguments
def parse_args():
	examples = """Examples:
python3 Icrawl --update
python3 Icrawl --dork --all --verbose
python3 Icrawl -u <url> --phone --hash --emails --verbose
python3 Icrawl -u <url> -A -r [0-9]{3}-[0-9]{3}-[0-9]{4} -v
python3 Icrawl -u <url> --regex ^\(\d{3}\)\s\d{3}-\d{4} --count 10"""
	
	parser = argparse.ArgumentParser(description=f"Information Crawler\t (Version : {__version__})\nAn osint web scraping and google dork tool", epilog=examples,formatter_class=argparse.RawDescriptionHelpFormatter)
	parser.add_argument("-A", "--all",     action="store_true",  help="Search for everything",               dest="all")
	parser.add_argument("-a", "--address", action="store_true",  help="Search for street addresses (Note: returns false positives)", dest="address")
	parser.add_argument("-ac", "--access", action="store_true",  help="Search for access tokens",            dest="access")
	parser.add_argument("-au", "--auth",   action="store_true",  help="Search for authentication tokens",    dest="auth")	
	parser.add_argument("-c", "--count",   metavar="" ,type=int, default=100 ,help="Maximum number of urls to scrape (Default 100)", dest="count")
	parser.add_argument('-d', '--dorks',   action="store_true", help="Run google searches using google dorks", dest="dorks")
	parser.add_argument("-e", "--emails",  action="store_true",  help="Search for emails addresses",         dest="emails")
	parser.add_argument("-H", "--hash",    action="store_true",  help="Search for password hashes",          dest="hash")
	parser.add_argument("-k", "--key",     action="store_true",  help="Search for api keys",                 dest="api")
	parser.add_argument("-l", "--lookup",  action="store_true",  help="Proform whois lookup and iplocation", dest="lookup")
	parser.add_argument("-o", "--output",  metavar="",type=str,  default="",   help="Save output to a file", dest="output")
	parser.add_argument("-p", "--phone",   action="store_true",  help="Search for phone numbers",            dest="phone")
	parser.add_argument("-r", "--regex",   metavar="",default=[],nargs='+',help="Use custom regular expressions",   dest="reg")
	parser.add_argument("-s", "--secrets", action="store_true",  help="Search for API keys, Hashes, Access tokens Authentication tokens", dest="secrets")
	parser.add_argument('-u', '--url', metavar="",type=str, default="", help="Url to start scraping/spidering", dest="url")
	parser.add_argument('-U', '--update', action="store_true",  help="Download the latest google dorks", dest="update")
	parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output", dest="verbose")
	args = parser.parse_args()
	
	if args.url == "" and args.dorks == False and args.update == False:
		parser.print_usage()
		exit()


	if args.dorks and args.update:
		parser.error("Can not run google dorks search and update at the same time")


	# Making sure user input for custom regexs are all valid regexs before moving on
	if args.reg != []:
		for reg in args.reg:
			try:
				formated_reg = fr"{reg}"
				compile(formated_reg)
			except:
				print(f"{error} Invalid regular expression {red}{reg}{norm}")
				exit(1)

	return args

# Print resaults from searches
def resaults(dframes, scraped_url):
	
	# Print urls and dataframes containing information scraped from that url
	if len(dframes) == 0:
		print(f"{error} No resaults found")
		if args.output != "":
			with open(args.output, "a") as f:
				f.write(f"{error} No resaults found\n")
	else:
		num = 0
		for i in dframes:
			for key in i:
				print(f"{blue}{key}{norm}")
				print(i[key])
				if args.output != "":
					with open(args.output, "a") as f:
						f.write(f"{blue}{key}{norm}\n")
						f.write(f"{i[key]}\n\n")

	# Print resaults from whois lookup
	if len(domain_infomation) > 0:
		for i in domain_infomation:
			print(i)
			if args.output != "":
				with open(args.output, "a") as f:
					f.write(i + "\n")
	
	# Print resaults from ip geolocation
	if len(ip_info) > 0:
		for key, value in ip_info:
			print(f"{green}[+] {cyan}{key}{norm}: {value}")
			if args.output != "":
				with open(args.output, "a") as f:
					f.write(f"{green}[+] {cyan}{key}{norm}: {value}\n")
	
	# Take user input and print all scraped urls
	if args.verbose:
		answer = input(f"[+] Do you want to show all {len(scraped_url)} crawled urls?(Y/N): ")
		if answer.lower() == "y":
			for i in scraped_url:
				print(f"{blue}{i}{norm}")
	
	# Writes scraped urls to a file
	if args.output != "":
		with open(args.output, "a") as f:
			f.write("\n")
			for i in scraped_url:
				f.write(f"{blue}{i}{norm}\n")

# Ask user if they want to scrape googles search resaults
def google_dorks():

	found_links = []
	dorks_to_use, number_of_links, timeout, = dork_payload.get_payload()
	new_links = run_dorks.run_search(dorks_to_use, number_of_links, timeout)
	found_links = found_links + new_links
		
	if len(found_links) != 0:
		print(f"{error} Warning: Scraping google search resaults can be against googles terms of service")
	
		CONTINUE = input("[+] Do you want to continue?(Y/N): ")			
		if CONTINUE.lower() == "y":
			ROOT_LOGGER.info("Parsing found links from google dorks")
			regex_resaults = []
			scraped_links = []
			banner = banners.get_banner()
			print(banner)
			print(f"{box} Icrawl has started")
			print(f"{box} Press CTRL-C to stop crawling")
			for link in found_links:
				args.url = link
				new_resaults, scraped_urls = run_crawler.crawler(args, False)
				regex_resaults = regex_resaults + new_resaults
				scraped_links = scraped_links + scraped_urls
				
			resaults(regex_resaults, scraped_links)
		else:			
			ROOT_LOGGER.info(f"Printing found links from google dorks and quitting")
			print()
			for url in found_links:
				print(url)
			print("Goodbye")
			exit()

# Get ip geolocation info and whois lookup info		
def lookup():
	base_url = args.url

	if args.verbose:
		print(f"{ibox} Running whois lookup")
	domain_infomation = Lookup.domain_infomation(base_url)
	url = base_url.split(".")
	url = f"{url[1]}.{url[2]}"
	if args.verbose:
		print(f"{ibox} Running IP geolocation lookup")
	domain_ip = gethostbyname(url)
	ip_info = Lookup.ip_lookup(domain_ip)

	return domain_infomation, ip_info



if __name__ == '__main__':
	args = parse_args()
	
	if args.lookup or args.all:
		domain_infomation, ip_info = lookup()
	else:
		domain_infomation = []
		ip_info = []

	
	if args.update:
		downloaded_dorks = dork_update.update_google_dorks()
		ROOT_LOGGER.info(f"Google dorks updated successfully {downloaded_dorks} where downloaded")
		if downloaded_dorks == None:
			downloaded_dorks = 0
			print(f"[!] {downloaded_dorks} where downloaded please check your internet connection")
		else:
			print(f"\n[+] {downloaded_dorks} google dorks where successfully downloaded")
	
	elif args.dorks:
		google_dorks()
	
	else: 
		banner = banners.get_banner()
		print(banner)
		print(f"{box} Icrawl has started")
		print(f"{box} Press CTRL-C to stop crawling")
		new_resaults, scraped_urls = run_crawler.crawler(args, True)
		resaults(new_resaults, scraped_urls)



